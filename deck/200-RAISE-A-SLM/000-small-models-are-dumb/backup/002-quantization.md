---
marp: true
theme: default
paginate: true
---

<style>
.dodgerblue {
  color: dodgerblue;
}
</style>
## Quantization üßÆ

- **Precision Reduction**: Technique that reduces numerical precision of model weights from 32-bit floats to lower bit representations (16-bit, 8-bit, or even 4-bit)
- **Size Benefits**: Converting from float32 to int8 can reduce model size by 4x, <span class="dodgerblue">**making large models fit on limited hardware**</span>
- **Performance Gains**: Lower precision computations are <span class="dodgerblue">**faster and use less memory**</span>, enabling quicker inference times
- **Quality Trade-off**: Slight accuracy loss in exchange for dramatic efficiency improvements, but modern techniques (GPTQ, GGML) minimize this impact
- **Deployment Enabler**: Allows running larger, more capable models on consumer hardware that couldn't handle full-precision versions


<!--
La quantization d'un mod√®le est une technique qui consiste √† r√©duire la
  pr√©cision num√©rique des poids et activations du mod√®le pour diminuer sa
  taille et acc√©l√©rer l'inf√©rence.

  Au lieu d'utiliser des nombres √† virgule flottante sur 32 bits (float32),
  on utilise des repr√©sentations plus compactes comme 16 bits, 8 bits, ou
  m√™me 4 bits. Par exemple, passer de float32 √† int8 divise par 4 la taille
  du mod√®le.

  Cette compression permet :
  - De r√©duire l'utilisation m√©moire
  - D'acc√©l√©rer les calculs
  - De faire tourner des mod√®les plus gros sur du mat√©riel limit√©

  Le compromis est une l√©g√®re perte de pr√©cision, mais les techniques
  modernes (comme GPTQ, GGML) maintiennent g√©n√©ralement de bonnes
  performances.
-->

[‚Üê Previous](../002-temperature.md) | [Next ‚Üí](../../001-context-this-is-the-way/demos/0-bot-draft/000-personality-and-soul.md)
