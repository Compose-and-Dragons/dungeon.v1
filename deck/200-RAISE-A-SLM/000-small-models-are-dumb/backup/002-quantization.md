---
marp: true
theme: default
paginate: true
---

<style>
.dodgerblue {
  color: dodgerblue;
}
</style>
## Quantization ðŸ§®

- **Precision Reduction**: Technique that reduces numerical precision of model weights from 32-bit floats to lower bit representations (16-bit, 8-bit, or even 4-bit)
- **Size Benefits**: Converting from float32 to int8 can reduce model size by 4x, <span class="dodgerblue">**making large models fit on limited hardware**</span>
- **Performance Gains**: Lower precision computations are <span class="dodgerblue">**faster and use less memory**</span>, enabling quicker inference times
- **Quality Trade-off**: Slight accuracy loss in exchange for dramatic efficiency improvements, but modern techniques (GPTQ, GGML) minimize this impact
- **Deployment Enabler**: Allows running larger, more capable models on consumer hardware that couldn't handle full-precision versions


<!--
La quantization d'un modÃ¨le est une technique qui consiste Ã  rÃ©duire la
  prÃ©cision numÃ©rique des poids et activations du modÃ¨le pour diminuer sa
  taille et accÃ©lÃ©rer l'infÃ©rence.

  Au lieu d'utiliser des nombres Ã  virgule flottante sur 32 bits (float32),
  on utilise des reprÃ©sentations plus compactes comme 16 bits, 8 bits, ou
  mÃªme 4 bits. Par exemple, passer de float32 Ã  int8 divise par 4 la taille
  du modÃ¨le.

  Cette compression permet :
  - De rÃ©duire l'utilisation mÃ©moire
  - D'accÃ©lÃ©rer les calculs
  - De faire tourner des modÃ¨les plus gros sur du matÃ©riel limitÃ©

  Le compromis est une lÃ©gÃ¨re perte de prÃ©cision, mais les techniques
  modernes (comme GPTQ, GGML) maintiennent gÃ©nÃ©ralement de bonnes
  performances.
-->